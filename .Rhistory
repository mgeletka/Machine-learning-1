Diab2<-PimaIndiansDiabetes2;
###################################################################
# MICE
sapply(Diab2, function(x) sum(is.na(x))) #sapply returns a list of the same length as X, each element
#of which is the result of applying FUN to the corresponding
#element of X.sapply is a user-friendly version and
#install.packages("mice")
library(mice)
init = mice(Diab2, maxit=0)
init
meth=init$method
predM=init$predictorMatrix
#meth[c("glucose","pressure","triceps", "insulin","mass")] <-"norm"
# run the multiple imputation
set.seed(101)
imputed = mice(Diab2, method=meth, predictorMatrix=predM, m=5)   # Q: is better to use pmm or norm method?
imput<-complete(imputed)
# Split the dataset in train (80%) and test (20%)
sample<-sample.split(imput,SplitRatio=0.8)
train.set<-subset(imput, sample==TRUE)
test.set <- subset(imput, sample==FALSE)
train.x <- train.set[,1:8]
test.x <- test.set[, 1:8]
train.y <- train.set[,9]
test.y <- test.set[,9]
classes <- factor(train.set$diabetes)
predicted.classes <- knn(train.x, test.x, classes, k=3)
# Error
ks <- c(1,3,5,7,9,11,15,17,23,25,35,45,55,83,101,151)
nks <- length(ks)
misclass.train <- numeric(length=nks)   # 0 vector of doubles
misclass.test <- numeric(length=nks)
names(misclass.test) <- ks
for (i in seq(along=ks)){
mod.train <- knn(train.x, train.x,classes, k=ks[i])
mod.test <- knn(train.x, test.x, classes, k=ks[i])
misclass.train[i] <- sum(mod.train!=train.y)/nrow(train.set)
misclass.test[i] <- sum(mod.test!=test.y)/nrow(test.set)
}
#x11()
plot(misclass.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n",ylim=c(0,0.4)) #type="n" non disegna nulla, xaxt="n" non mette valori sull'asse x
axis(1, 1:length(ks), as.character(ks))
lines(misclass.test,type="b",col='blue',pch=20)
lines(misclass.train,type="b",col='red',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
title('Test performace vs Train performace')
col1<-c('blue', 'magenta')
library(grid)  #ci serve per plottare una griglia
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x8 <- seq(min(imput[,8]), max(imput[,8]), 1)
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
contour(grid.x2, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
plot.knn(3)
sample<-sample.split(imput,SplitRatio=0.8)
train.set<-subset(imput, sample==TRUE)
test.set <- subset(imput, sample==FALSE)
sample<-sample.split(imput,SplitRatio=0.8)
train.set<-subset(imput, sample==TRUE)
test.set <- subset(imput, sample==FALSE)
train.x <- train.set[,1:8]
test.x <- test.set[, 1:8]
train.y <- train.set[,9]
test.y <- test.set[,9]
classes <- factor(train.set$diabetes)
predicted.classes <- knn(train.x, test.x, classes, k=3)
col1<-c('blue', 'magenta')
col1<-c('blue', 'magenta')
library(grid)
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x8 <- seq(min(imput[,8]), max(imput[,8]), 1)
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
contour(grid.x2, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
remove(list=ls());
library(mlbench)
# install.packages("caTools")
library(caTools)
utils::data(PimaIndiansDiabetes2)
Diab2<-PimaIndiansDiabetes2;
###################################################################
# MICE
sapply(Diab2, function(x) sum(is.na(x))) #sapply returns a list of the same length as X, each element
#of which is the result of applying FUN to the corresponding
#element of X.sapply is a user-friendly version and
#install.packages("mice")
library(mice)
init = mice(Diab2, maxit=0)
init
meth=init$method
predM=init$predictorMatrix
#meth[c("glucose","pressure","triceps", "insulin","mass")] <-"norm"
# run the multiple imputation
set.seed(101)
imputed = mice(Diab2, method=meth, predictorMatrix=predM, m=5)   # Q: is better to use pmm or norm method?
imput<-complete(imputed)
sample<-sample.split(imput,SplitRatio=0.8)
train.set<-subset(imput, sample==TRUE)
test.set <- subset(imput, sample==FALSE)
train.x <- train.set[,1:8]
test.x <- test.set[, 1:8]
train.y <- train.set[,9]
test.y <- test.set[,9]
classes <- factor(train.set$diabetes)
predicted.classes <- knn(train.x, test.x, classes, k=3)
col1<-c('blue', 'magenta')
library(grid)
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x8 <- seq(min(imput[,8]), max(imput[,8]), 1)
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
contour(grid.x2, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
help(contour.default)
predicted.classes <- knn(train.x, grid, classes, k=3)
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x8 <- seq(min(imput[,8]), max(imput[,8]), 1)
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
colnames(grid) <- colnames(train.x)
kneighbours=3
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
View(predicted.matrix)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
View(predicted.matrix)
predicted.classes
predicted.classes <- knn(train.x, test.x, classes, k=3)
predicted.classes
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
predicted.classes
predicted.classes <- knn(train.x, test.x, classes, k=kneighbours)
predicted.classes
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
View(grid)
colnames(grid) <- colnames(train.x)
View(grid)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
init = mice(Diab2, maxit=0)
init
meth=init$method
predM=init$predictorMatrix
#meth[c("glucose","pressure","triceps", "insulin","mass")] <-"norm"
# run the multiple imputation
set.seed(103)
imputed = mice(Diab2, method=meth, predictorMatrix=predM, m=5)   # Q: is better to use pmm or norm method?
imput<-complete(imputed)
# Split the dataset in train (80%) and test (20%)
sample<-sample.split(imput,SplitRatio=0.8)
train.set<-subset(imput, sample==TRUE)
test.set <- subset(imput, sample==FALSE)
train.x <- train.set[,1:8]
test.x <- test.set[, 1:8]
train.y <- train.set[,9]
test.y <- test.set[,9]
classes <- factor(train.set$diabetes)
col1<-c('blue', 'magenta')
library(grid)
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x8 <- seq(min(imput[,8]), max(imput[,8]), 1)
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
contour(grid.x2, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
plot.knn(3)
plot.knn <- function(kneighbours){
grid.x3 <- seq(min(imput[,3]), max(imput[,3]), 1)
grid.x8 <- seq(min(imput[,8]), max(imput[,8]), 1)
grid <- expand.grid(0,0, grid.x3,0,0,0,0,grid.x8)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$pressure, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='pressure', ylab='age')
points(grid$pressure, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x3), length(grid.x8))
contour(grid.x3, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
kneighbours=1
grid.x3 <- seq(min(imput[,3]), max(imput[,3]), 1)
imput
o<-Diab2[,-9]
o<-imput[,-9]
###############
lm(imput$diabetes~., data=imput[,-9])
###############
lm(imput$diabetes~., data=imput)
###############
imput.x<-imput[,-9]
lm(imput$diabetes~., data=imput.x)
View(imput.x)
factor(iris$Species)
plot.knn(1)
lm(as.numeric(imput$diabetes)~., data=imput.x)
lm(as.numeric(imput$diabetes)~., data=imput)
lm(as.numeric(imput$diabetes)~., data=imput[,-9])
ml<-lm(as.numeric(imput$diabetes)~., data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant,imput$glucose, imput$pressure, imput$triceps, imput$insulin,imput$mass,imput$pedigree,imput$age, data=imput[,-9])
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+ imput$triceps+ imput$insulin+imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+  imput$insulin+imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+  imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+ imput$triceps+ imput$insulin+imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+  imput$insulin+imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$mass+imput$pedigree, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+  imput$insulin+imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$pressure+ imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$mass+imput$pedigree+imput$age, data=imput[,-9])
summary(ml)
ml<-lm(as.numeric(imput$diabetes)~imput$pregnant+imput$glucose+ imput$mass+imput$pedigree, data=imput[,-9])
summary(ml)
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(0,grid.x2,0,0,0,grid.x6,0,0)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$pressure, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(0,grid.x2,0,0,0,grid.x6,0,0)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
plot.knn(3)
plot.knn(5)
plot.knn(10)
plot.knn(3)
grid <- expand.grid(0,grid.x2,0,0,0,grid.x6,0,0)
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(0,grid.x2,0,0,0,grid.x6,0,0)
grid
View(grid)
mean(imput[,1])
grid <- expand.grid(mean(imput[,1]),grid.x2,mean(imput[,3]),mean(imput[,4]),mean(imput[,5]),grid.x6,mean(imput[,7]),mean(imput[,8]))
View(grid)
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
predicted.classes
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(mean(imput[,1]),grid.x2,mean(imput[,3]),mean(imput[,4]),mean(imput[,5]),grid.x6,mean(imput[,7]),mean(imput[,8]))
colnames(grid) <- colnames(train.x)
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
plot.knn(3)
plot.knn(5)
plot.knn(10)
plot.knn(1)
plot.knn(8)
as.numeric(imput$diabetes)
View(imput)
View(imput)
f_col <- function(x){ if (x<1.5) 'blue' else 'magenta'}
lsm <- function(regmod, data){
val <- predict(regmod, data)
sapply(val,f_col)
}
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(mean(imput[,1]),grid.x2,mean(imput[,3]),mean(imput[,4]),mean(imput[,5]),grid.x6,mean(imput[,7]),mean(imput[,8]))
colnames(grid) <- colnames(train.x)
predicted.classes <- lsm(ml,grid)
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with linear regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
####################################################
# PCA
#install.packages("ggfortify")
library(ggfortify)
library(devtools)
install_github("vqv/ggbiplot")
pca<- prcomp(imput[,-9], center = TRUE,scale. = TRUE)
pca.values <- pca$x
pca <- princomp (train.x, cor = TRUE, scores = TRUE)
str(pca)
test.pred <- cbind(test.x, predicted.classes)
autoplot(prcomp(test.x), data = test.pred, colour = 'predicted.classes')
# Decision boudaries in PCA
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 0.1)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 0.1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(train.x, grid, classes, k=kneighbours)
x11()
plot(imput$glucose, imput$age, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
contour(grid.x2, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
f_col <- function(x){ if (x<1.5) 'blue' else 'magenta'}
lsm <- function(regmod, data){
val <- predict(regmod, data)
sapply(val,f_col)
}
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(mean(imput[,1]),grid.x2,mean(imput[,3]),mean(imput[,4]),mean(imput[,5]),grid.x6,mean(imput[,7]),mean(imput[,8]))
colnames(grid) <- colnames(train.x)
predicted.classes <- lsm(ml,grid)
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("Classification with linear regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
predicted.classes <- lsm(ml,grid)
View(imput)
grid <- expand.grid(mean(imput[,1]),grid.x2,grid.x6,mean(imput[,7]))
colnames(grid) <- colnames(train.x[c(1,2,6,7)])
View(grid)
predicted.classes <- lsm(ml,grid)
val <- predict(ml,grid)
val <- predict(ml,c(1,2),c(2,5),c(0,0),c(3,100))
val <- predict(ml,data.frame(c(1,2),c(2,5),c(0,0),c(3,100)))
val <- predict(ml,data.frame(pregnant=c(1,2),glucose=c(2,5),mass=c(0,0),pedigree=c(3,100)))
d <- data.frame(x1=rnorm(80,6,2.5), x2=NA, y=0)
d$x2 <- 7 - 8/16* d$x1 + rnorm(nrow(d), 0, 1)
d1<- data.frame(x1=rnorm(80, 5,2.5), x2=NA, y=1)
d1$x2<- rnorm(nrow(d1), 8.5, 2)
tot<- rbind(d, d1)
val <- predict(m, data.frame(x1=c(4,4), x2=c(6,6.735)))
m<-lm(tot$y ~ ., tot)
val <- predict(m, data.frame(x1=c(4,4), x2=c(6,6.735)))
View(tot)
View(tot)
m<-lm(tot$y ~ tot$x1+tot$x2, tot)
val <- predict(m, data.frame(x1=c(4,4), x2=c(6,6.735)))
val1 <- predict(m, data.frame(x1=c(4,4), x2=c(6,6.735)))
m<-lm(tot$y ~ ., tot)
val <- predict(m, data.frame(x1=c(4,4), x2=c(6,6.735)))
m1 <-lm(tot$y ~ tot$x1+tot$x2, tot)
View(m)
View(m1)
m1 <-lm(tot$y ~ x1+x2, tot)
val1 <- predict(m, data.frame(x1=c(4,4), x2=c(6,6.735)))
ml<-lm(as.numeric(imput$diabetes)~pregnant+glucose+ mass+pedigree, data=imput[,-9])
lsm <- function(regmod, data){
val <- predict(regmod, data)
sapply(val,f_col)
}
f_col <- function(x){ if (x<1.5) 'blue' else 'magenta'}
lsm <- function(regmod, data){
val <- predict(regmod, data)
sapply(val,f_col)
}
grid.x2 <- seq(min(imput[,2]), max(imput[,2]), 1)
grid.x6 <- seq(min(imput[,6]), max(imput[,6]), 1)
grid <- expand.grid(mean(imput[,1]),grid.x2,grid.x6,mean(imput[,7]))
colnames(grid) <- colnames(train.x[c(1,2,6,7)])
predicted.classes <- lsm(ml,grid)
x11()
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
grid$glucose
grid$mass
points(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
View(grid)
plot(grid$glucose, grid$mass, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
plot(grid$glucose, grid$mass)# pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass)# pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.')#, col=col1[as.numeric(predicted.classes)])  # draw grid
predicted.classes <- lsm(ml,grid)
predicted.classes
val<-predict(ml,grid)
pp<-sapply(val,f_col)
pp[979]
plot(imput$glucose, imput$mass, pch=20, col=col1[as.numeric(imput$diabetes)], xlab='glucose', ylab='mass')
points(grid$glucose, grid$mass, pch='.', col=col1[predicted.classes])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
points(grid$glucose, grid$mass, pch='.', col=predicted.classes)  # draw grid
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
round(1.5)
round(1.49)
lsm[pr.cl, val] <- function(regmod, data){
val <- round(predict(regmod, data))
pr.cl<-sapply(val,f_col)
}
lsm[pr.cl, val] <- function(regmod, data){
val <- round(predict(regmod, data))
pr.cl<-sapply(val,f_col)
}
f_col <- function(x){ if (x<1.5) 'blue' else 'magenta'}
lsm[pr.cl, val] <- function(regmod, data){
val <- round(predict(regmod, data))
pr.cl<-sapply(val,f_col)
}
summary(predicted.classes)
as.numeric(predicted.classes)
data.frame(predicted.classes)
d<-data.frame(predicted.classes)
View(d)
d$predicted.classes
k<-d$predicted.classes
kk<-as.numeric(k)
kk
pr.cl.val <- data.frame(predicted.classes)$predicted.classes
pr.cl.val <- as.numeric(data.frame(predicted.classes)$predicted.classes)
predicted.matrix <- matrix(pr.cl.val, length(grid.x2), length(grid.x6))
contour(grid.x2, grid.x6, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
library(arules);
install.packages("arules");
install.packages("arulesViz",dependencies = TRUE);
library(arules);
library(arulesViz);
