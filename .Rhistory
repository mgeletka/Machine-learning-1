pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train.set$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(trainDf$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(imput$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(trainDf$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, trainDf$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(trainDf$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(trainDf$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(5)
pca <- prcomp (trainDf)
str(testDf)
library(ggfortify)
autoplot(prcomp(testDf[,-9]), data = testDf, colour = 'pred.classes')
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(5)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(model1, cp = bestcp)
conf.matrix <- table(data$diabetes, predict(model1.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes ~., data = trainDf, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes ~.,data = testDf, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes ~., data = trainDf, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes ~.,data = testDf, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(data$diabetes ~., data = trainDf, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(data$diabetes ~.,data = testDf, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes~., data = trainDf, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes~.,data = testDf, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pima")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(model1, cp = bestcp)
conf.matrix <- table(data$diabetes, predict(model1.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes ~., data = trainDf, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes ~.,data = testDf, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes ~ ., data = trainDf, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes ~ .,data = testDf, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes ~ ., data = data, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes ~ .,data = data, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
misclass.test
# Figure 2.4
plot(misclass.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 0.4))
axis(1, 1:length(ks), as.character(ks))
lines(misclass.test,type="b",col='blue',pch=20)
lines(misclass.train,type="b",col='red',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
ks <- c(0.5,1,1.5,2)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- rpart(diabetes ~ ., data = data, method = "class", control = rpart.control(cp = ks[i]))
mod.test <- rpart(diabetes ~ .,data = data, method = 'class', control = rpart.control(cp = ks[i]))
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
misclass.test
# Figure 2.4
plot(misclass.train,xlab="Number of cp",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 0.4))
axis(1, 1:length(ks), as.character(ks))
lines(misclass.test,type="b",col='blue',pch=20)
lines(misclass.train,type="b",col='red',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
library("mlbench")
library("caTools")
library("class")
library(mice)
library("rpart")
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
pca.values <- pca$x
plot.knn <- function(kneighbours){
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(5)
col1<-c('blue', 'magenta')
plot.knn <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class")
predicted.classes <- predict(model1, testDf, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1,2,8)
library("mlbench")
library("caTools")
library("class")
library(mice)
library("rpart")
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
model1 <- rpart(diabetes ~., data = data, method = "class")
str(model1)
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
predicted.classes <- predict(model1, testDf, type = "class")
print(1 - mean(predicted.classes == test$diabetes))
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Diabetess")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(model1, cp = bestcp)
conf.matrix <- table(data$diabetes, predict(model1.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
col1<-c('blue', 'magenta')
plot.knn <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class")
predicted.classes <- predict(model1, testDf, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1,2,8)
plot.knn(3,5,6)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class")
predicted.classes <- predict(model1, testDf, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
plot.cart(3,5,6)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 1000))
predicted.classes <- predict(model1, testDf, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
plot.cart(3,5,6)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 100))
predicted.classes <- predict(model1, testDf, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
plot.cart(3,5,6)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 100))
predicted.classes <- predict(model1, train, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
plot.cart(3,5,6)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 100))
predicted.classes <- predict(model1, train, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.classes.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 100))
predicted.classes <- predict(model1, grid(), type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.classes, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 100))
predicted.classes <- predict(model1, grid, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.classes, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
library("mlbench")
library("caTools")
library("class")
library(mice)
library("rpart")
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
col1<-c('blue', 'magenta')
plot.cart <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
model1 <- rpart(diabetes ~., data = data, method = "class", control = rpart.control(minbucket = 100))
predicted.classes <- predict(model1, grid, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
print(predicted.classes)
contour(grid.A, grid.B, predicted.classes, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.cart(1,2,8)
