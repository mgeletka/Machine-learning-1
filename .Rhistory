axis(1, 1:length(ks), as.character(ks))
lines(precision.train,type="b",col='red',pch=20)
lines(precision.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(recall.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Recall")
axis(1, 1:length(ks), as.character(ks))
lines(recall.train,type="b",col='red',pch=20)
lines(recall.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
col1<-c('blue', 'magenta')
plot.knn <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
predicted.classes <- knn(trainDf[, c(indexA, indexB)], grid, train$diabetes, k=k)
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(15, 2, 8)
plot.knn(23, 3, 6)
plot.knn(23, 4, 5)
plot.knn(15, 8, 7)
res.pca <- prcomp(data[,1:8], scale = TRUE)
plot.knn.pca <- function(k) {
pca.values <- prcomp(trainDf)$x
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=k)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn.pca(1)
plot.knn.pca(7)
plot.knn.pca(15)
plot.knn.pca(23)
plot.knn.pca(50)
plot.knn.pca(100)
glm <- glm(diabetes~.,family=binomial(logit),data=train)
summary(glm)
glm2 <- glm(diabetes~pregnant + glucose + mass + pedigree,family=binomial(logit),data=train)
summary(glm2)
predicted.classes.train <- predict(glm2, trainDf, type = "response")
predicted.classes.train <- sapply(predicted.classes.train, function(x) round(x) + 1)
confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
logr.accuracy.train <- (confussionMatrixTrain[1,1] + confussionMatrixTrain[2,2]) / length(predicted.classes.train)
logr.precision.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
logr.recall.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
predicted.classes.test <- predict(glm2, testDf, type = "response")
predicted.classes.test <- sapply(predicted.classes.test, function(x) round(x) + 1)
confussionMatrixTest <- table(predicted.classes.test, test$diabetes)
logr.accuracy.test <- (confussionMatrixTest[1,1] + confussionMatrixTest[2,2]) / length(predicted.classes.test)
logr.precision.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
logr.recall.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
plot.logRegression.boundaries <- function(indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
glm2 <- glm(diabetes~.,family=binomial(logit),data = train[, c(indexA, indexB, 9)])
predicted.classes <- predict(glm2, grid, type = "response")
predicted.classes <- lapply(predicted.classes, function(x) round(x) + 1)
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with logistic regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.logRegression.boundaries(2, 8)
plot.logRegression.boundaries(3, 6)
plot.logRegression.boundaries(4, 5)
plot.logRegression.boundaries(8, 7)
plot.logRegression.pca <- function() {
pca.values <- prcomp(trainDf)$x
pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
pca.df$diabetes <- train$diabetes
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
glm2 <- glm(diabetes~.,family=binomial(logit),data=pca.df)
predicted.classes <- predict(glm2, grid, type = "response")
predicted.classes <- lapply(predicted.classes, function(x) round(x) + 1)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification of logistic regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.logRegression.pca()
possible_bucket_sizes <- seq(1, 300, 10)
nks <- length(possible_bucket_sizes)
accuracy.train <- numeric(length=nks)
accuracy.test <- numeric(length=nks)
precision.train <- numeric(length=nks)
precision.test <- numeric(length=nks)
recall.train <- numeric(length=nks)
recall.test <- numeric(length=nks)
for(i in seq(1:length(possible_bucket_sizes))){
cart_model <- rpart(diabetes ~., data = train, method = "class",control = rpart.control(minbucket = possible_bucket_sizes[i]))
predicted.classes.test <- predict(cart_model, testDf, type = "class")
predicted.classes.train <- predict(cart_model, trainDf, type = "class")
confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
accuracy.train[i] <- mean(predicted.classes.train == train$diabetes)
precision.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
recall.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
confussionMatrixTest = table(predicted.classes.test, test$diabetes)
accuracy.test[i] <- mean(predicted.classes.test == test$diabetes)
precision.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
recall.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
}
plot(accuracy.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Accuracy",)
axis(1, 1:length(ks), as.character(ks))
lines(accuracy.train,type="b",col='red',pch=20)
lines(accuracy.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(precision.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Precision",)
axis(1, 1:length(ks), as.character(ks))
lines(precision.train,type="b",col='red',pch=20)
lines(precision.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(recall.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Recall",)
axis(1, 1:length(ks), as.character(ks))
lines(recall.train,type="b",col='red',pch=20)
lines(recall.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot.treeBoundaries <- function(mb, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
cart_model <- rpart(diabetes ~ . , data = train[, c(indexA, indexB, 9)], method = "class",control = rpart.control(minbucket = mb))
predicted.classes <- predict(cart_model, grid, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with minibucket size=", mb))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.treeBoundaries(23, 2, 8)
plot.treeBoundaries(23,3, 6)
plot.treeBoundaries(23,4, 5)
plot.treeBoundaries(23, 8, 7)
plot.treeBoundaries.pca <- function(mb) {
pca.values <- prcomp(trainDf)$x
pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
pca.df$diabetes <- train$diabetes
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
cart_model <- rpart(diabetes ~., data = pca.df, method = "class",control = rpart.control(minbucket = mb))
predicted.classes <- predict(cart_model, grid, type = "class")
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with mini bucket size =", mb))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.treeBoundaries.pca(1)
plot.treeBoundaries.pca(7)
plot.treeBoundaries.pca(15)
plot.treeBoundaries.pca(23)
plot.treeBoundaries.pca(35)
plot.treeBoundaries.pca(50)
ml<-lm(as.numeric(diabetes)~pregnant+glucose+pressure+triceps+insulin+mass+pedigree+age, data=train)
summary(ml)
ml2<-lm(as.numeric(diabetes)~pregnant+glucose+ mass+pedigree, data=train)
summary(ml2)
predicted.classes.train <- predict(ml2, trainDf, type = "response")
predicted.classes.train <- sapply(predicted.classes.train, function(x) round(x))
confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
linreg.accuracy.train <- (confussionMatrixTrain[1,1] + confussionMatrixTrain[2,2]) / length(predicted.classes.train)
linreg.precision.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
linreg.recall.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
predicted.classes.test <- predict(ml2, testDf, type = "response")
predicted.classes.test <- sapply(predicted.classes.test, function(x) round(x))
confussionMatrixTest <- table(predicted.classes.test, test$diabetes)
linreg.accuracy.test <- (confussionMatrixTest[1,1] + confussionMatrixTest[2,2]) / length(predicted.classes.test)
linreg.precision.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
linreg.recall.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
plot.linreg.boundaries <- function(indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
ml <- lm(as.numeric(diabetes)~.,data = train[, c(indexA, indexB, 9)])
predicted.classes <- round(predict(ml, grid, type = "response"))
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with linear regression"))
predicted.matrix <- matrix(predicted.classes, length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.linreg.boundaries(2, 8)
plot.linreg.boundaries(3, 6)
plot.linreg.boundaries(4, 5)
plot.linreg.boundaries(8, 7)
plot.linreg.pca <- function() {
pca.values <- prcomp(trainDf)$x
pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
pca.df$diabetes <- train$diabetes
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
ml <- lm(as.numeric(diabetes)~.,data=pca.df)
predicted.classes <- round(predict(ml, grid, type = "response"))
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA classification of linear regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.linreg.pca()
library("mlbench")
library("caTools")
library("class")
library(mice)
library(grid)
library("rpart")
library(GGally)
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
data <- complete(mice(data, method=init$method, predictorMatrix=init$predictorMatrix, m=5))
p <- ggpairs(data[sample(1:nrow(data),100),], columns = 1:8, ggplot2::aes(colour=diabetes), lower = list(continuous=wrap("points")), progress=FALSE,legend=1)
p
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
trainDf <- train[, -9]
testDf <- test[,-9]
#NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
#trainDf <- replace(trainDf, TRUE, lapply(trainDf, NA2mean))
#testDf <- replace(testDf, TRUE, lapply(testDf, NA2mean))
#pred.classes <- knn(trainDf, testDf, train$diabetes, k=5)
#testDf <- cbind(testDf,pred.classes)
ks <- c(1,3,5,7,9,11,15,17,23,25,35,45,55)# nearest neighbours to try
nks <- length(ks)
accuracy.train <- numeric(length=nks)
accuracy.test <- numeric(length=nks)
precision.train <- numeric(length=nks)
precision.test <- numeric(length=nks)
recall.train <- numeric(length=nks)
recall.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- knn(trainDf,trainDf,k=ks[i],cl=train$diabetes)
confussionMatrixTrain = table(mod.train, train$diabetes)
accuracy.train[i] = sum(mod.train == train$diabetes)/length(train$diabetes)
precision.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
recall.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
mod.test <- knn(trainDf, testDf[,-9],k= ks[i],cl= train$diabetes)
confussionMatrixTest = table(mod.test, test$diabetes)
accuracy.test[i] = sum(mod.test == test$diabetes)/length(test$diabetes)
precision.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
recall.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
}
plot(accuracy.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Accuracy")
axis(1, 1:length(ks), as.character(ks))
lines(accuracy.train,type="b",col='red',pch=20)
lines(accuracy.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(precision.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Precision")
axis(1, 1:length(ks), as.character(ks))
lines(precision.train,type="b",col='red',pch=20)
lines(precision.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(recall.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Recall")
axis(1, 1:length(ks), as.character(ks))
lines(recall.train,type="b",col='red',pch=20)
lines(recall.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
col1<-c('blue', 'magenta')
plot.knn <- function(k, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
predicted.classes <- knn(trainDf[, c(indexA, indexB)], grid, train$diabetes, k=k)
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(15, 2, 8)
plot.knn(23, 3, 6)
plot.knn(23, 4, 5)
plot.knn(15, 8, 7)
res.pca <- prcomp(data[,1:8], scale = TRUE)
plot.knn.pca <- function(k) {
pca.values <- prcomp(trainDf)$x
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=k)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with k=", k))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn.pca(1)
plot.knn.pca(7)
plot.knn.pca(15)
plot.knn.pca(23)
plot.knn.pca(50)
plot.knn.pca(100)
glm <- glm(diabetes~.,family=binomial(logit),data=train)
summary(glm)
glm2 <- glm(diabetes~pregnant + glucose + mass + pedigree,family=binomial(logit),data=train)
summary(glm2)
predicted.classes.train <- predict(glm2, trainDf, type = "response")
predicted.classes.train <- sapply(predicted.classes.train, function(x) round(x) + 1)
confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
logr.accuracy.train <- (confussionMatrixTrain[1,1] + confussionMatrixTrain[2,2]) / length(predicted.classes.train)
logr.precision.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
logr.recall.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
predicted.classes.test <- predict(glm2, testDf, type = "response")
predicted.classes.test <- sapply(predicted.classes.test, function(x) round(x) + 1)
confussionMatrixTest <- table(predicted.classes.test, test$diabetes)
logr.accuracy.test <- (confussionMatrixTest[1,1] + confussionMatrixTest[2,2]) / length(predicted.classes.test)
logr.precision.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
logr.recall.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
plot.logRegression.boundaries <- function(indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
glm2 <- glm(diabetes~.,family=binomial(logit),data = train[, c(indexA, indexB, 9)])
predicted.classes <- predict(glm2, grid, type = "response")
predicted.classes <- lapply(predicted.classes, function(x) round(x) + 1)
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with logistic regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.logRegression.boundaries(2, 8)
plot.logRegression.boundaries(3, 6)
plot.logRegression.boundaries(4, 5)
plot.logRegression.boundaries(8, 7)
plot.logRegression.pca <- function() {
pca.values <- prcomp(trainDf)$x
pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
pca.df$diabetes <- train$diabetes
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
glm2 <- glm(diabetes~.,family=binomial(logit),data=pca.df)
predicted.classes <- predict(glm2, grid, type = "response")
predicted.classes <- lapply(predicted.classes, function(x) round(x) + 1)
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification of logistic regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.logRegression.pca()
possible_bucket_sizes <- seq(1, 300, 10)
nks <- length(possible_bucket_sizes)
accuracy.train <- numeric(length=nks)
accuracy.test <- numeric(length=nks)
precision.train <- numeric(length=nks)
precision.test <- numeric(length=nks)
recall.train <- numeric(length=nks)
recall.test <- numeric(length=nks)
for(i in seq(1:length(possible_bucket_sizes))){
cart_model <- rpart(diabetes ~., data = train, method = "class",control = rpart.control(minbucket = possible_bucket_sizes[i]))
predicted.classes.test <- predict(cart_model, testDf, type = "class")
predicted.classes.train <- predict(cart_model, trainDf, type = "class")
confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
accuracy.train[i] <- mean(predicted.classes.train == train$diabetes)
precision.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
recall.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
confussionMatrixTest = table(predicted.classes.test, test$diabetes)
accuracy.test[i] <- mean(predicted.classes.test == test$diabetes)
precision.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
recall.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
}
plot(accuracy.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Accuracy",)
axis(1, 1:length(ks), as.character(ks))
lines(accuracy.train,type="b",col='red',pch=20)
lines(accuracy.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(precision.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Precision",)
axis(1, 1:length(ks), as.character(ks))
lines(precision.train,type="b",col='red',pch=20)
lines(precision.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot(recall.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Recall",)
axis(1, 1:length(ks), as.character(ks))
lines(recall.train,type="b",col='red',pch=20)
lines(recall.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
plot.treeBoundaries <- function(mb, indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
cart_model <- rpart(diabetes ~ . , data = train[, c(indexA, indexB, 9)], method = "class",control = rpart.control(minbucket = mb))
predicted.classes <- predict(cart_model, grid, type = "class")
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with minibucket size=", mb))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.treeBoundaries(23, 2, 8)
plot.treeBoundaries(23,3, 6)
plot.treeBoundaries(23,4, 5)
plot.treeBoundaries(23, 8, 7)
plot.treeBoundaries.pca <- function(mb) {
pca.values <- prcomp(trainDf)$x
pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
pca.df$diabetes <- train$diabetes
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
cart_model <- rpart(diabetes ~., data = pca.df, method = "class",control = rpart.control(minbucket = mb))
predicted.classes <- predict(cart_model, grid, type = "class")
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA Classification with mini bucket size =", mb))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.treeBoundaries.pca(1)
plot.treeBoundaries.pca(7)
plot.treeBoundaries.pca(15)
plot.treeBoundaries.pca(23)
plot.treeBoundaries.pca(35)
plot.treeBoundaries.pca(50)
ml<-lm(as.numeric(diabetes)~pregnant+glucose+pressure+triceps+insulin+mass+pedigree+age, data=train)
summary(ml)
ml2<-lm(as.numeric(diabetes)~pregnant+glucose+ mass+pedigree, data=train)
summary(ml2)
predicted.classes.train <- predict(ml2, trainDf, type = "response")
predicted.classes.train <- sapply(predicted.classes.train, function(x) round(x))
confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
linreg.accuracy.train <- (confussionMatrixTrain[1,1] + confussionMatrixTrain[2,2]) / length(predicted.classes.train)
linreg.precision.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
linreg.recall.train <- confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
predicted.classes.test <- predict(ml2, testDf, type = "response")
predicted.classes.test <- sapply(predicted.classes.test, function(x) round(x))
confussionMatrixTest <- table(predicted.classes.test, test$diabetes)
linreg.accuracy.test <- (confussionMatrixTest[1,1] + confussionMatrixTest[2,2]) / length(predicted.classes.test)
linreg.precision.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
linreg.recall.test <- confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
plot.linreg.boundaries <- function(indexA, indexB) {
grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
grid <- expand.grid(grid.A,grid.B)
colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
ml <- lm(as.numeric(diabetes)~.,data = train[, c(indexA, indexB, 9)])
predicted.classes <- round(predict(ml, grid, type = "response"))
plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA], ylab=colnames(data)[indexB])
points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with linear regression"))
predicted.matrix <- matrix(predicted.classes, length(grid.A), length(grid.B))
contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.linreg.boundaries(2, 8)
plot.linreg.boundaries(3, 6)
plot.linreg.boundaries(4, 5)
plot.linreg.boundaries(8, 7)
plot.linreg.pca <- function() {
pca.values <- prcomp(trainDf)$x
pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
pca.df$diabetes <- train$diabetes
grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
grid <- expand.grid(grid.PC1,grid.PC2)
colnames(grid) <- c('PC1','PC2')
ml <- lm(as.numeric(diabetes)~.,data=pca.df)
predicted.classes <- round(predict(ml, grid, type = "response"))
plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("PCA classification of linear regression"))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.linreg.pca()
