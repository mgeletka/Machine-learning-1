library("mlbench")
library("caTools")
library("class")
library(mice)
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
trainDf <- train[, -9]
testDf <- test[,-9]
#NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
#trainDf <- replace(trainDf, TRUE, lapply(trainDf, NA2mean))
#testDf <- replace(testDf, TRUE, lapply(testDf, NA2mean))
pred.classes <- knn(trainDf, testDf, train$diabetes, k=3)
testDf <- cbind(testDf,pred.classes)
trainDf <- train[, -9]
testDf <- test[,-9]
#NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
#trainDf <- replace(trainDf, TRUE, lapply(trainDf, NA2mean))
#testDf <- replace(testDf, TRUE, lapply(testDf, NA2mean))
pred.classes <- knn(trainDf, testDf, train$diabetes, k=3)
testDf <- cbind(testDf,pred.classes)
pca <- princomp (trainDf, cor = TRUE, scores = TRUE)
str(testDf)
library(ggfortify)
autoplot(prcomp(testDf[,-9]), data = testDf, colour = 'pred.classes')
ks <- c(1,3,5,7,9,11,13,15,17,19,21,23,50,70, 90, 150, 200,250,300,350)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- knn(trainDf,trainDf,k=ks[i],cl=train$diabetes)
mod.test <- knn(trainDf, testDf[,-9],k= ks[i],cl= train$diabetes)
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
misclass.test
# Figure 2.4
plot(misclass.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 0.4))
axis(1, 1:length(ks), as.character(ks))
lines(misclass.test,type="b",col='blue',pch=20)
lines(misclass.train,type="b",col='red',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
col1 <- c('blue', 'magenta')
library(grid)
plot.knn <- function(kneighbours){
grid.gen.x1 <- seq(min(data[,:4]),max(data[,:8]),0.1)
col1<-c('blue', 'magenta')
library(grid)  #ci serve per plottare una griglia
plot.knn <- function(kneighbours){
grid.x2 <- seq(min(data[,2]), max(data[,2]), 1)
grid.x8 <- seq(min(data[,8]), max(data[,8]), 1)
grid <- expand.grid(0, grid.x2,0,0,0,0,0,grid.x8)
colnames(grid) <- colnames(trainDf)
predicted.classes <- knn(trainDf, grid, data$diabetes, k=kneighbours)
x11()
plot(data$glucose, data$age, pch=20, col=col1[as.numeric(data$diabetes)], xlab='glucose', ylab='age')
points(grid$glucose, grid$age, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
legend("topleft", legend=levels(data$diabetes),fill =col1)
title(c("Classification with k=", kneighbours))
predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.x2), length(grid.x8))
contour(grid.x2, grid.x8, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.knn(1)
glm <- glm(diabetes~.,family=binomial(logit),data=data)
summary(glm)
glm2 <- glm(diabetes~pregnant + glucose + insulin + mass + pedigree,family=binomial(logit),data=data)
summary(glm2)
glm3 <- glm(diabetes~pregnant + glucose + mass + pedigree,family=binomial(logit),data=data)
summary(glm3)
(x = c(min(data[,1])-2,  max(data[,1])+2))
(y = c((-1/coefs[3]) * (coefs[2] * x + coefs[1])))
coef = coef(glm2)
(x = c(min(data[,1])-2,  max(data[,1])+2))
(y = c((-1/coefs[3]) * (coefs[2] * x + coefs[1])))
coefs = coef(glm2)
(x = c(min(data[,1])-2,  max(data[,1])+2))
(y = c((-1/coefs[3]) * (coefs[2] * x + coefs[1])))
lines(x, y, col="black", lwd=2)
deviationFromZero <- function(y) abs(predict(model, data.frame(x = y)))
boundary <- optimize(f = deviationFromZero, interval = range(data$diabetes))
model1 <- rpart(diabetes ~., data = trainDf, method = "class")
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(caret)
library(rpart)
model1 <- rpart(diabetes ~., data = trainDf, method = "class")
library(tidyverse)
library(caret)
library(rpart)
model1 <- rpart(data$diabetes ~., data = trainDf, method = "class")
model1 <- rpart(diabetes ~., data = trainDf, method = "class")
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
predicted.classes <- model1 %>%
predict(test.data, type = "class")
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
head(predicted.classes)
pca <- princomp (trainDf, cor = TRUE, scores = TRUE)
str(testDf)
library(ggfortify)
autoplot(prcomp(testDf[,-9]), data = testDf, colour = 'pred.classes')
library("mlbench")
library("caTools")
library("class")
library(mice)
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
model1 <- rpart(diabetes ~., data = data, method = "class")
library("mlbench")
library("caTools")
library("class")
library(mice)
library("rpart")
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
model1 <- rpart(diabetes ~, data = data, method = "class")
model1 <- rpart(diabetes ~pregnant + glucose + insulin + mass + pedigree, data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
model1 <- rpart(diabetes ~pregnant + glucose + insulin + mass + pedigree, data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
predicted.classes <- model1 %>%
predict(test.data, type = "class")
model1 <- rpart(diabetes ~pregnant + glucose + insulin + mass + pedigree, data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
mean(predicted.classes == test.data$diabetes)
model1 <- rpart(diabetes ~pregnant + glucose + insulin + mass + pedigree, data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
mean(predicted.classes == data$diabetes)
predicted.classes <- model1 %>%
predict(test.data, type = "class")
model1 <- rpart(diabetes ~pregnant + glucose + insulin + mass + pedigree, data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
mean(predicted.classes == data$diabetes)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
model1 <- rpart(diabetes ~pregnant + glucose + insulin + mass + pedigree, data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
mean(predicted.classes == data$diabetes)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
mean(predicted.classes == data$diabetes)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
library("mlbench")
library("caTools")
library("class")
library(mice)
library("rpart")
utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2
set.seed(101)
init = mice(data, maxit=0)
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)
sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)
trainDf <- train[, -9]
testDf <- test[,-9]
#NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
#trainDf <- replace(trainDf, TRUE, lapply(trainDf, NA2mean))
#testDf <- replace(testDf, TRUE, lapply(testDf, NA2mean))
pred.classes <- knn(trainDf, testDf, train$diabetes, k=3)
testDf <- cbind(testDf,pred.classes)
ks <- c(1,3,5,7,9,11,13,15,17,19,21,23,50,70, 90, 150, 200,250,300,350)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
mod.train <- knn(trainDf,trainDf,k=ks[i],cl=train$diabetes)
mod.test <- knn(trainDf, testDf[,-9],k= ks[i],cl= train$diabetes)
misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}
misclass.test
# Figure 2.4
plot(misclass.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 0.4))
axis(1, 1:length(ks), as.character(ks))
lines(misclass.test,type="b",col='blue',pch=20)
lines(misclass.train,type="b",col='red',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
mean(predicted.classes == data$diabetes)
predicted.classes <- model1 %>%
predict(testDf, type = "class")
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postscript plot of tree
post(model1, file = "c:/tree.ps",
title = "Classification Tree for Kyphosis")
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
model1 <- rpart(diabetes ~., data = data, control = rpart.control(minsplit = 20),method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
model1 <- rpart(diabetes ~., data = data, control = rpart.control(minsplit = 20),method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
model1 <- rpart(diabetes ~., data = data, control = rpart.control(maxcompete = 4),method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
model1 <- rpart(diabetes ~., data = data, control = rpart.control(maxdepth = 4),method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
conf.matrix <- table(ptitanic$survived, predict(tree.pruned,type="class"))
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(tree, cp = bestcp)
model1 <- rpart(diabetes ~., data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(model1, cp = bestcp)
conf.matrix <- table(data$diabetes, predict(model1.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
model1 <- rpart(diabetes ~pregnant + glucose + mass + pedigree, data = data, method = "class")
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
# plot tree
plot(model1, uniform=TRUE,
main="Classification Tree for Pina")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(model1, cp = bestcp)
conf.matrix <- table(data$diabetes, predict(model1.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
