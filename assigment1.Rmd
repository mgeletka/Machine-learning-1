---
title: "Machine learning 1 assigment"
output: html_notebook
---

Loading and diving the data

```{r}
library("mlbench")
library("caTools")
library("class")
library(mice)

utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2

set.seed(101) 

init = mice(data, maxit=0) 
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)


sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)

```
KNN
```{r}
trainDf <- train[, -9]
testDf <- test[,-9]

#NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
#trainDf <- replace(trainDf, TRUE, lapply(trainDf, NA2mean))
#testDf <- replace(testDf, TRUE, lapply(testDf, NA2mean))

pred.classes <- knn(trainDf, testDf, train$diabetes, k=3)
testDf <- cbind(testDf,pred.classes)

```
PCA and plot
```{r}
pca <- princomp (trainDf, cor = TRUE, scores = TRUE)
str(testDf)

library(ggfortify)
autoplot(prcomp(testDf[,-9]), data = testDf, colour = 'pred.classes')
```
Error of the knn
```{r}
ks <- c(1,3,5,7,9,11,13,15,17,19,21,23,50,70, 90, 150, 200,250,300,350)
# nearest neighbours to try
nks <- length(ks)

accuracy.train <- numeric(length=nks)
accuracy.test <- numeric(length=nks)

precision.train <- numeric(length=nks)
precision.test <- numeric(length=nks)

recall.train <- numeric(length=nks)
recall.test <- numeric(length=nks)


for (i in seq(along=ks)) {
mod.train <- knn(trainDf,trainDf,k=ks[i],cl=train$diabetes)
confussionMatrixTrain = table(mod.train, train$diabetes)
accuracy.train[i] = sum(mod.train == train$diabetes)/length(train$diabetes)
precision.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
recall.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])

mod.test <- knn(trainDf, testDf[,-9],k= ks[i],cl= train$diabetes)
confussionMatrixTest = table(mod.test, test$diabetes)
accuracy.test[i] = sum(mod.test == test$diabetes)/length(test$diabetes)
precision.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
recall.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
}

#Accuracy
plot(accuracy.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 1),  main="Accuracy",)
axis(1, 1:length(ks), as.character(ks))
lines(accuracy.train,type="b",col='red',pch=20)
lines(accuracy.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))


#Precision
plot(precision.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 1),  main="Precision",)
axis(1, 1:length(ks), as.character(ks))
lines(precision.train,type="b",col='red',pch=20)
lines(precision.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))


#Recall
plot(recall.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 1),  main="Recall",)
axis(1, 1:length(ks), as.character(ks))
lines(recall.train,type="b",col='red',pch=20)
lines(recall.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
```
Logistic regression
```{r}
glm <- glm(diabetes~.,family=binomial(logit),data=data)
summary(glm)
```
we see thath Significant are only pregnant, glucoe, intercept, mass and pedigree
```{r}
glm2 <- glm(diabetes~pregnant + glucose + insulin + mass + pedigree,family=binomial(logit),data=data)
summary(glm2)
```
now the insulin is not significant
```{r}
glm3 <- glm(diabetes~pregnant + glucose + mass + pedigree,family=binomial(logit),data=data)
summary(glm3)
```
