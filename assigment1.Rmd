---
title: "Machine learning 1 assigment"
output: html_notebook
---

Loading and diving the data

```{r}
library("mlbench")
library("caTools")
library("class")
library(mice)
library("rpart")

utils::data("PimaIndiansDiabetes2")
data <- PimaIndiansDiabetes2

set.seed(101) 

init = mice(data, maxit=0) 
meth = init$method
predM = init$predictorMatrix
imputed = mice(data, method=meth, predictorMatrix=predM, m=5)
data <- complete(imputed)


sample <- sample.split(data, SplitRatio = 0.8)
train <- subset(data, sample == TRUE)
test  <- subset(data, sample == FALSE)

```
KNN
```{r}
trainDf <- train[, -9]
testDf <- test[,-9]

#NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
#trainDf <- replace(trainDf, TRUE, lapply(trainDf, NA2mean))
#testDf <- replace(testDf, TRUE, lapply(testDf, NA2mean))

pred.classes <- knn(trainDf, testDf, train$diabetes, k=3)
testDf <- cbind(testDf,pred.classes)

print(pred.classes)
```
PCA and plot
```{r}
pca <- prcomp (trainDf)
str(testDf)
library(ggfortify)
autoplot(prcomp(testDf[,-9]), data = testDf, colour = 'pred.classes')
```
Error of the knn
```{r}
ks <- c(1,3,5,7,9,11,13,15,17,19,21,23,50,70, 90, 150, 200,250,300,350)
# nearest neighbours to try
nks <- length(ks)
misclass.train <- numeric(length=nks)
misclass.test <- numeric(length=nks)
for (i in seq(along=ks)) {
  mod.train <- knn(trainDf,trainDf,k=ks[i],cl=train$diabetes)
  mod.test <- knn(trainDf, testDf[,-9],k= ks[i],cl= train$diabetes)
  misclass.train[i] <- 1 - sum(mod.train==train$diabetes)/nrow(trainDf)
  misclass.test[i] <- 1 - sum(mod.test==test$diabetes)/nrow(testDf)
}

misclass.test
# Figure 2.4
plot(misclass.train,xlab="Number of NN",ylab="Test error",type="n",xaxt="n", ylim=c(0.0, 0.4))
axis(1, 1:length(ks), as.character(ks))
lines(misclass.test,type="b",col='blue',pch=20)
lines(misclass.train,type="b",col='red',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
```

Decision Bundaries
```{r}
col1<-c('blue', 'magenta')
plot.knn <- function(k, indexA, indexB) {
  grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
  grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
  grid <- expand.grid(grid.A,grid.B)
  colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
  predicted.classes <- knn(trainDf[, c(indexA, indexB)], grid, train$diabetes, k=k)
  plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
  points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("Classification with k=", k))
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
  print(predicted.classes)
  contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
  }

plot.knn(1,2,8)
plot.knn(3,5,6)
```
Decision Bundaries with PCA

```{r}
Boundaris <- function(kn){
  
  PCA1 <- pca$x[,1]
  PCA2 <- pca$x[,2]
  
  grid.A <- seq(min(PCA1), max(PCA1), (max(PCA1) - min(PCA1)) / 100)
  grid.B <- seq(min(PCA2), max(PCA2), (max(PCA2) - min(PCA2)) / 100)
  
  grid <- expand.grid(grid.A,grid.B)
  colnames(grid) <- colnames(c("PCA1","PCA2"))
  
  predicted.classes <- knn(pca$x[, c(1, 2)], grid, train$diabetes, k=kn)
  
  plot(PCA1, PCA2, pch=20, col=col1[as.numeric(data$diabetes)], xlab="PCA2",    ylab="PCA2")
  points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("Classification with k=", kn))
  
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
  contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
  }
  
Boundaris(3)
```




```{r}
pca.values <- pca$x

plot.knn <- function(kneighbours){
  grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
  grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
  grid <- expand.grid(grid.PC1,grid.PC2)
  colnames(grid) <- c('PC1','PC2')
  predicted.classes <- knn(pca.values[,c(1,2)], grid, train$diabetes, k=kneighbours)
  plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
  points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("PCA Classification with k=", kneighbours))
  
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
  contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}

plot.knn(5)



```








Logistic regression
```{r}
glm <- glm(diabetes~.,family=binomial(logit),data=data)
```








CART
```{r}
model1 <- rpart(diabetes ~., data = data, method = "class")
str(model1)
# Plot the trees
printcp(model1) # display the results
plotcp(model1) # visualize cross-validation results
summary(model1) # detailed summary of splits
predicted.classes <- predict(model1, testDf, type = "class")
print(1 - mean(predicted.classes == test$diabetes))
# plot tree
plot(model1, uniform=TRUE,
   main="Classification Tree for Diabetess")
text(model1, use.n=TRUE, all=TRUE, cex=.8)
bestcp <- model1$cptable[which.min(model1$cptable[,"xerror"]),"CP"]
model1.pruned <- prune(model1, cp = bestcp)
conf.matrix <- table(data$diabetes, predict(model1.pruned,type="class"))
rownames(conf.matrix) <- paste("Actual", rownames(conf.matrix), sep = ":")
colnames(conf.matrix) <- paste("Pred", colnames(conf.matrix), sep = ":")
print(conf.matrix)
```


```{r}
possible_bucket_sizes <- seq(1, 300, 10)
nks <- length(possible_bucket_sizes)
accuracy.train <- numeric(length=nks)
accuracy.test <- numeric(length=nks)
precision.train <- numeric(length=nks)
precision.test <- numeric(length=nks)
recall.train <- numeric(length=nks)
recall.test <- numeric(length=nks)
for(i in seq(1:length(possible_bucket_sizes))){
  cart_model <- rpart(diabetes ~., data = train, method = "class",control = rpart.control(minbucket = possible_bucket_sizes[i]))
  predicted.classes.test <- predict(cart_model, testDf, type = "class")
  predicted.classes.train <- predict(cart_model, trainDf, type = "class")
    confussionMatrixTrain = table(predicted.classes.train, train$diabetes)
    accuracy.train[i] <- mean(predicted.classes.train == train$diabetes)
    precision.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[,1])
    recall.train[i] = confussionMatrixTrain[1,1]/sum(confussionMatrixTrain[1,])
    confussionMatrixTest = table(predicted.classes.test, test$diabetes)
    accuracy.test[i] <- mean(predicted.classes.test == test$diabetes)
    precision.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[,1])
    recall.test[i] = confussionMatrixTest[1,1]/sum(confussionMatrixTest[1,])
}

#Accuracy
plot(accuracy.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Accuracy",)
axis(1, 1:length(ks), as.character(ks))
lines(accuracy.train,type="b",col='red',pch=20)
lines(accuracy.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
#Precision
plot(precision.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Precision",)
axis(1, 1:length(ks), as.character(ks))
lines(precision.train,type="b",col='red',pch=20)
lines(precision.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
#Recall
plot(recall.train,xlab="Number of items in leafs",ylab="Test error",type="n",xaxt="n", ylim=c(0.5, 1),  main="Recall",)
axis(1, 1:length(ks), as.character(ks))
lines(recall.train,type="b",col='red',pch=20)
lines(recall.test,type="b",col='blue',pch=20)
legend("bottomright",lty=1,col=c("red","blue"),legend = c("train ", "test "))
```


```{r}
plot.treeBoundaries.pca <- function(mb) {
  pca.values <- prcomp(trainDf)$x
  pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
  pca.df$diabetes <- train$diabetes
  
  grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
  grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
  grid <- expand.grid(grid.PC1,grid.PC2)
  colnames(grid) <- c('PC1','PC2')
    
  cart_model <- rpart(diabetes ~., data = pca.df, method = "class",control = rpart.control(minbucket = mb))
  predicted.classes <- predict(cart_model, grid, type = "class")
  
  
  plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
  points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("PCA Classification with mini bucket size =", mb))
  
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
  contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.treeBoundaries.pca(100)

```



```{r}

col1<-c('blue', 'magenta')
plot.cart <- function(indexA, indexB) {
  
  grid.A <- seq(min(data[,indexA]), max(data[,indexA]), (max(data[,indexA]) - min(data[,indexA])) / 100)
  grid.B <- seq(min(data[,indexB]), max(data[,indexB]), (max(data[,indexB]) - min(data[,indexB])) / 100)
  grid <- expand.grid(grid.A,grid.B)
  
  colnames(grid) <- colnames(trainDf[, c(indexA, indexB)])
  
  model1 <- rpart(diabetes ~., data = data[,c(indexA,indexB,9)], method = "class")

  

  predicted.classes <- predict(model1, grid, type = "class")
  
  
  plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
  points(grid[, 1], grid[,2], pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title("Decision Bounderies")
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.A), length(grid.B))
  print(predicted.classes)
  contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
  }

plot.cart(2,8)
```

##########################################################################################
                                  Linear regression
##########################################################################################
```{r}
  data.x<-data[,-9]
  
  ml<-lm(as.numeric(data$diabetes)~pregnant+glucose+pressure+triceps+insulin+mass+pedigree+age, data=data[,-9])
  summary(ml)
  ml<-lm(as.numeric(data$diabetes)~pregnant+glucose+ pressure+  insulin+mass+pedigree+age, data=data[,-9])
  summary(ml)
  ml<-lm(as.numeric(data$diabetes)~pregnant+glucose+ pressure+ mass+pedigree+age, data=data[,-9])
  summary(ml)
  ml<-lm(as.numeric(data$diabetes)~pregnant+glucose+ mass+pedigree+age, data=data[,-9])
  summary(ml)
  ml<-lm(as.numeric(data$diabetes)~pregnant+glucose+ mass+pedigree, data=data[,-9])
  summary(ml)
```
# classifier
```{r}
  f_col <- function(x){ if (x<1.5) 'blue' else 'magenta'}
  
  lsm <- function(regmod, data){
    val <- round(predict(regmod, data))
    pr.cl<-sapply(val,f_col)
  }
```
# Boundaries
```{r}

plot.lr <- function(indexA,indexB){
  grid.A <- seq(min(data[,indexA]), max(data[,indexA]), 1)
  grid.B <- seq(min(data[,indexB]), max(data[,indexB]), 1)
  grid <- expand.grid(grid.A,grid.B)
  colnames(grid) <- colnames(train[c(indexA,indexB)])
  
  ml<-lm(as.numeric(data$diabetes)~., data = data[,c(indexA,indexB)])

  predicted.classes <- lsm(ml,grid)
  pr.cl.val <- as.numeric(data.frame(predicted.classes)$predicted.classes)

  plot(data[, indexA], data[ ,indexB], pch=20, col=col1[as.numeric(data$diabetes)], xlab=colnames(data)[indexA],     ylab=colnames(data)[indexB])
  points(grid[,1], grid[,2], pch='.', col=predicted.classes)  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("Classification with linear regression"))
  
  predicted.matrix <- matrix(pr.cl.val, length(grid.A), length(grid.B))
  contour(grid.A, grid.B, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE) # but it would be more reasonable draw a linear function
}

plot.lr(3,6)
```

PCA linear 

```{r}
plot.linear.pca <- function() {
  pca.values <- prcomp(trainDf)$x
  pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
  pca.df$diabetes <- train$diabetes
  
  grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
  grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
  grid <- expand.grid(grid.PC1,grid.PC2)
  colnames(grid) <- c('PC1','PC2')
    
  ml<-lm(as.numeric(train$diabetes)~., data = pca.df)

  predicted.classes <- lsm(ml,grid)

  pr.cl.val <- as.numeric(data.frame(predicted.classes)$predicted.classes)
  plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')

  points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("PCA Classification with mini bucket size ="))
  
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
  contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}

plot.linear.pca()
```
```{r}
plot.linRegression.pca <- function() {
  pca.values <- prcomp(trainDf)$x
  pca.df <- as.data.frame.matrix(pca.values[,c(1,2)])
  pca.df$diabetes <- train$diabetes
  
  grid.PC1 <- seq(min(pca.values[,1]), max(pca.values[,1]), 5)
  grid.PC2 <- seq(min(pca.values[,2]), max(pca.values[,2]), 1)
  grid <- expand.grid(grid.PC1,grid.PC2)
  colnames(grid) <- c('PC1','PC2')
    
  glm3 <- lm(diabetes~.,data=pca.df)
  predicted.classes <- predict(glm3, grid, type = "response")
  predicted.classes <- lapply(predicted.classes, function(x) round(x) + 1)
  
  
  plot(pca.values[,1], pca.values[,2], pch=20, col=col1[as.numeric(train$diabetes)], xlab='PC1', ylab='PC2')
  points(grid$PC1, grid$PC2, pch='.', col=col1[as.numeric(predicted.classes)])  # draw grid
  legend("topleft", legend=levels(data$diabetes),fill =col1)
  title(c("PCA Classification of logistic regression"))
  
  predicted.matrix <- matrix(as.numeric(predicted.classes), length(grid.PC1), length(grid.PC2))
  contour(grid.PC1, grid.PC2, predicted.matrix, levels=c(1.5), drawlabels=FALSE,add=TRUE)
}
plot.linRegression.pca()
```







